<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Tensorflow结合kubeflow进行分布式训练 | xigang's home</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Tensorflow结合kubeflow进行分布式训练</h1><a id="logo" href="/.">xigang's home</a><p class="description">Do it right or don't do it at all</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Tensorflow结合kubeflow进行分布式训练</h1><div class="post-meta">Jan 30, 2019<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="post-content"><p>Tensorflow是Google在2015年11月开源的机器学习框架，来源了Google内部的深度学习框架DistBelief。由于其良好的架构，分布式架构支持以简单易用，自开源以来得到广泛的关注。</p>
<a id="more"></a>
<h2 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h2><h3 id="Tensorflow是什么"><a href="#Tensorflow是什么" class="headerlink" title="Tensorflow是什么"></a>Tensorflow是什么</h3><p>Tensorflow 是一个采用数据流图(<a href="https://en.wikipedia.org/wiki/Data_flow_diagram" target="_blank" rel="noopener">data flow graphs</a>)技术来进行数值计算的开源软件库。</p>
<p><div align="left"><br><img src="http://p4.qhimg.com/t012e840bf2840c3241.gif" width="250" height="400" alt="tensorflow"></div></p>
<p>其中Tensor代表传递的数据为张量(多维数组), Flow代表使用计算图进行运算。数据流图是一个有向图，使用<code>节点</code>(nodes) 和<code>线</code>(edges)来描述数学计算。<code>节点</code>一般用来表示数学操作(operation)，但也可以表示数据输入的起点和输出的终点。<code>边</code>表示节点之间的输入/输出关系。这些数据边可以传送维度可动态调整的多维数据数组，即<code>张量(Tensor)</code>。</p>
<h3 id="Tensorflow的特点"><a href="#Tensorflow的特点" class="headerlink" title="Tensorflow的特点"></a>Tensorflow的特点</h3><p>优点:</p>
<ul>
<li>由Google开发、维护，因此可以保障支持、开发的持续性。</li>
<li>简单易用，并且社区还有很多的模型封装(比如: keras和skflow等)</li>
<li>灵活高效，即可以使用CPU，也可以使用GPU</li>
<li>开放活跃的社区</li>
<li>支持网络训练的低级，高级接口</li>
</ul>
<p>缺点:</p>
<ul>
<li>计算图是纯Python的，因此速度很慢</li>
<li>图构造是静态的，意味着图必须先被编译在运行</li>
</ul>
<h3 id="Tensorflow的基础知识"><a href="#Tensorflow的基础知识" class="headerlink" title="Tensorflow的基础知识"></a>Tensorflow的基础知识</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p><a href="https://www.tensorflow.org/guide/tensors" target="_blank" rel="noopener">张量</a>是对矢量和矩阵向潜在的更高维度的泛化。Tensorflow在内部将张量表示为基本数据类型的n维数组。可以简单的将它理解为一个多维数组:</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3</span>                           # 这个是<span class="number">0</span>阶张量，就是标量，shape = []</span><br><span class="line">[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>]                  # 这个是<span class="number">1</span>阶张量，就是向量，shape = [<span class="number">3</span>]</span><br><span class="line">[[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>]]     # 这个是<span class="number">2</span>阶张量，就是二维数组，shape = [<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">[[[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>]],[[<span class="number">7.</span>,<span class="number">8.</span>,<span class="number">9.</span>]]] # 这个是<span class="number">3</span>阶张量，就是三维数组，shape = [<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<p>TensorFlow内部使用tf.Tensor类的实例来表示张量，每个tf.Tensor有两个属性：</p>
<ul>
<li>dtype Tensor: 存储的数据类型，可以为<code>tf.float32</code>,<code>tf.int32</code>, <code>tf.string</code>。</li>
<li>shape Tensor: 存储的多维数组中每个维度的数组的元素的个数。</li>
</ul>
<p>通过一个简单的示例来看下Tensorflow的张量:<br>注意:使用前需要安装<a href="https://www.tensorflow.org/install/" target="_blank" rel="noopener">Tensorflow</a></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Python <span class="number">2.7</span>.<span class="number">12</span> (default, Dec  <span class="number">4</span> <span class="number">2017</span>, <span class="number">14</span><span class="symbol">:</span><span class="number">50</span><span class="symbol">:</span><span class="number">18</span>)</span><br><span class="line">[GCC <span class="number">5.4</span>.<span class="number">0</span> <span class="number">20160609</span>] on linux2</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import tensorflow as tf</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; tensor<span class="number">0</span>=tf.constant(<span class="number">3</span>, dtype=tf.int32)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; tensor1=tf.constant([<span class="number">3</span>.,<span class="number">4.1</span>,<span class="number">5.2</span>], dtype=tf.float32)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; tensor2=tf.constant([[<span class="string">'apple'</span>,<span class="string">'orange'</span>],[<span class="string">'potato'</span>,<span class="string">'tomato'</span>]], dtype=tf.string)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; tensor3=tf.constant([[[<span class="number">5</span>], [<span class="number">6</span>], [<span class="number">7</span>]], [[<span class="number">4</span>], [<span class="number">3</span>], [<span class="number">2</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; print(tensor<span class="number">0</span>)</span><br><span class="line">Tensor(<span class="string">"Const:0"</span>, shape=(), dtype=int32)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; print(tensor1)</span><br><span class="line">Tensor(<span class="string">"Const_1:0"</span>, shape=(<span class="number">3</span>,), dtype=float32)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; print(tensor2)</span><br><span class="line">Tensor(<span class="string">"Const_2:0"</span>, shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=string)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; print(tensor3)</span><br><span class="line">Tensor(<span class="string">"Const_3:0"</span>, shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), dtype=int32)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="计算图-Graph-和会话-Session"><a href="#计算图-Graph-和会话-Session" class="headerlink" title="计算图(Graph)和会话(Session)"></a>计算图(Graph)和会话(Session)</h4><p>Tensorflow的主要特色就是<code>计算图</code>方法。基本上所有的Tensorflow代码都包含两个重要的部分:</p>
<ul>
<li>创建<code>计算图</code>，表示计算的数据流</li>
<li>运行<code>会话</code>，执行图中的运算</li>
</ul>
<h5 id="计算图-Graph"><a href="#计算图-Graph" class="headerlink" title="计算图(Graph)"></a>计算图(Graph)</h5><p>如官网所说: 一个计算图是被组织到图节点的一系列Tensorflow运算。首先，什么是节点和运算？最好的解释方式是，举个例子。假设我们为函数 <code>f(x,y)=x^2y+y+2</code> 编写代码。Tensorflow中的计算图如下所示:</p>
<p><div align="left"><br><img src="http://p2.qhimg.com/t019aa5d5ebb3b9dc02.png" width="350" height="250" alt="tensorflow"></div></p>
<p>如上图所示，计算图有一系列由边互相连接的节点构成。每个节点称为<code>op</code>,即<code>operation</code>的缩写。因此每个节点代表一个运算，可能是张量运算或生成张量的操作。每个节点以零或者更多的张量为输入，并生成一个张量作为输出。</p>
<p>我们来构建一个简单的计算图:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; <span class="selector-tag">a</span> = <span class="number">2</span></span><br><span class="line">&gt;&gt;&gt; <span class="selector-tag">b</span> = <span class="number">3</span></span><br><span class="line">&gt;&gt;&gt; c = tf.add(<span class="selector-tag">a</span>, <span class="selector-tag">b</span>, name=<span class="string">'Add'</span>)</span><br><span class="line">&gt;&gt;&gt; print(c)</span><br><span class="line"><span class="function"><span class="title">Tensor</span><span class="params">(<span class="string">"Add:0"</span>, shape=()</span></span>, dtype=int32)</span><br></pre></td></tr></table></figure>
<p>上面的代码已经将计算图构建完成。但是并不能对其进行执行。我们可以把上面的计算图比作Python中的<code>函数定义</code>。它不会为你执行任何计算(就像函数定义不会有任何的执行结果)，它定义计算的操作。</p>
<h5 id="会话-Session"><a href="#会话-Session" class="headerlink" title="会话(Session)"></a>会话(Session)</h5><p>在Tensorflow中，所有不同的变量和运算都存储在计算图。所以在我们构建完模型所需要的图之后，还需要打开一个会话(Session)来运行整个计算图。在会话中，我们可以将所有的计算分配到可用的CPU和GPU资源中。举个简单的例子，运行计算图并获取c的值：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; sess = tf.Session()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span>(sess.run(c))</span></span><br><span class="line">5</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; sess.close()</span></span><br></pre></td></tr></table></figure>
<p>这些代码创建一个Session()对象，然后调用<code>run</code>方法来运行上面的计算图。计算完毕之后需要关闭会话来帮助系统回收资源。</p>
<h3 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h3><p>为了方便Tensorflow的建模和调优，Google还为Tensorflow开发了一款可视化的工具：<code>TensorBoard</code>。我们通过一个简单的现象线性回归模型来看下TessorBoard。</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># linear_model: y = W*x + b</span></span><br><span class="line">W = tf.Variable([.1], dtype=tf.float32)</span><br><span class="line">b = tf.Variable([-.1], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, name='x')</span><br><span class="line">y = tf.placeholder(tf.float32, name='y')</span><br><span class="line"></span><br><span class="line"><span class="comment"># create linear model</span></span><br><span class="line">linear_model = W * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># create loss model</span></span><br><span class="line">with tf.name_scope(<span class="string">"loss-model"</span>):</span><br><span class="line">    loss = tf.reduce_sum(tf.square(linear_model -y))</span><br><span class="line">    <span class="comment">#Add scalar to the output of the loss model to observe the convergence curve of loss</span></span><br><span class="line">    tf.summary.scalar(<span class="string">"loss"</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a optimizer use Gradient Descent algorithm.</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(0.001)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create session use compute</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># merges all sunmaries collected in the default graph</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment">#All data generated by the model run is saved to the /tmp/tensorflow folder for use by TensorBoard</span></span><br><span class="line">writer = tf.summary.FileWriter('/tmp/tensorflow', sess.graph)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train dataset</span></span><br><span class="line">x_train = [1, 2, 3, 6, 8]</span><br><span class="line">y_train = [4.8, 8.5, 10.4, 21.0, 25.3]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training 10,000 times</span></span><br><span class="line">for i in range(10000):</span><br><span class="line">    <span class="comment"># Pass in merge during training</span></span><br><span class="line">    summary, _ = sess.run([merged, train], &#123;x: x_train, y: y_train&#125;)</span><br><span class="line">    <span class="comment"># collected output train data</span></span><br><span class="line">    writer.add_summary(summary, i)</span><br><span class="line"></span><br><span class="line">current_W, current_b, current_loss = sess.run([W, b, loss], &#123;x: x_train, y: y_train&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the results after training</span></span><br><span class="line">print(<span class="string">"After train W: %s b: %s, loss: %s"</span> % (current_W, current_b, current_loss))</span><br></pre></td></tr></table></figure>
<p>运行上面的代码后，训练过程产生的数据就保存在<code>/tmp/tensorflow</code>文件夹下。可以使用下面的命令来启动TensorBoard:</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard <span class="params">--logdir</span> <span class="string">/tmp/tensorflow</span></span><br></pre></td></tr></table></figure>
<p>通过浏览器的<code>http://localhost:6006</code>就能看到我们的模型数据。在 SCALARS 页面我们可以看到我们通过 tf.summary.scalar(“loss”, loss)设置的loss收敛曲线，从曲线图中可以看出在训练了大概2000次的时候loss就已经收敛的差不多了。</p>
<p><div align="left"><br><img src="http://p9.qhimg.com/t0114119e4e59c10323.png" width="500" height="400" alt="tensorflow"></div></p>
<p>在 GRAPHS 页面可以看到我们构建的模型的数据流图： </p>
<p><div align="left"><br><img src="http://p6.qhimg.com/t01cb47fe4fbdc5c58b.png" width="650" height="410" alt="tensorflow"></div></p>
<h3 id="Tensorflow-分布式训练"><a href="#Tensorflow-分布式训练" class="headerlink" title="Tensorflow 分布式训练"></a>Tensorflow 分布式训练</h3><h4 id="分布式训练策略"><a href="#分布式训练策略" class="headerlink" title="分布式训练策略"></a>分布式训练策略</h4><ul>
<li><p>模型并行</p>
<p>所谓模型并行指的是将模型部署到很多设备上(设备可能分布在不同的机器上)运行，比如:多个机器的GPUs。当神经网络模型很大时，由于显存的限制，它是难以跑在单个GPU上，这个时候就需要模型并行。但是这种分布式训练方式不常使用。</p>
</li>
<li><p>数据并行<br>深度学习模型最常用的分布式训练策略是数据并行，因为训练时的一个重要原因是训练的数据量很大。数据并行就是在很多设备上放置相同的模型，并且各个设备采用不同的训练样本来对模型进行训练。训练深度学习模型常采用的是<code>batch SGD</code>方法，采用数据并行，可以每个设备都训练不同的batch，然后收集这些梯度用于模型参数的更新。采用数据并行策略，使用256个GPUs，每个GPU读取32个图片进行训练，如下图所示，这样相当于采用非常大的batch（ 32\times 256=8192 ）来训练模型。</p>
<p>  <div align="left"><br>  <img src="http://p5.qhimg.com/t0170b2f0d6ec9c7345.png" width="650" height="250" alt="tensorflow"></div></p>
<p>  <code>数据并行</code>可以是同步(synchronous),也可以是异步(asynchronous)。所谓同步指的是所有的设备都是采用相同的模型参数来训练，等待所有设备的batch训练完成后，收集它们的梯度然后取均值,然后执行模型的一次参数更新。这相当于通过聚合很多设备上的batch形成一个很大的batch来训练模型，通过这种方式能使学习速率得到不错的效果。同步训练看起来不错，但是实现需要各个设备的计算能力均衡，而且要求集群的通信也要均衡，类似<code>木桶效应</code>,一个拖油瓶会严重拖慢训练进度，所以同步训练方式相对来说训练速度会慢一些。<code>异步训练</code>中，各个设备完成一个mini-batch训练之后，不需要等待其它节点，直接去更新模型的参数，这样总体会训练速度会快很多。但是异步训练的一个很严重的问题是梯度失效问题（stale gradients），刚开始所有设备采用相同的参数来训练，但是异步情况下，某个设备完成一步训练后，可能发现模型参数其实已经被其它设备更新过了，此时这个梯度就过期了，因为现在的模型参数和训练前采用的参数是不一样的。由于梯度失效问题，异步训练虽然速度快，但是可能陷入次优解（sub-optimal training performance）。异步训练和同步训练在TensorFlow中不同点如下图所示：</p>
<p>   <div align="left"><br>  <img src="http://p8.qhimg.com/t01c7eb6b1278e53213.png" width="800" height="450" alt="tensorflow"></div></p>
</li>
</ul>
<h4 id="分布式训练介绍"><a href="#分布式训练介绍" class="headerlink" title="分布式训练介绍"></a>分布式训练介绍</h4><p>通过多GPU并行的方式可以有很好的加速效果，然后一台机器上所支持的GPU是有限的，因此需要分布式的训练方式来支持多机器的分布式训练。</p>
<h5 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h5><ul>
<li>客户端(client): 客户端是一个用于建立Tensorflow计算图并创立于集群进行交互的会话层(Session)的程序。一般客户端通过Python或C++实现。一个独立的客户端进程可以同时与多个Tensorflow的服务端相连，同时一个独立的服务端也可以与多个客户端相连。</li>
<li>集群(Cluster): 一个Tensorflow的集群里包含了一个或多个Job,每一个作业又可以拆分成一个或多个任务(task)。集群对象可以通过<code>tf.train.ClusterSpec</code>来定义。</li>
<li>作业(job): 一个作业可以拆分成多个具有相同目的的任务(task),比如说，一个称之为 ps(parameter server，参数服务器) 的作业中的任务主要是保存和更新变量，而一个名为 work(工作)的作业一般是管理无状态且主要从事计算的任务。一个作业中的任务可以运行于不同的机器上，作业的角色也是灵活可变的，比如说称之为”work”的作业可以保存一些状态。</li>
<li>主节点服务逻辑(master service): 一个RPC服务程序可以用来远程连接一系列的分布式设备，并扮演一个会话终端的角色，主服务程序实现了一个tensorflow:session的接口并负责通过工作节点的服务进程与工作的任务进行通信。所有的主服务程序都有了主节点的服务逻辑。</li>
<li>任务(Task): 任务相当于一个特定的Tensorflow服务端，其相当于一个独立的进程，该进程属于特定的作业并在作业找那个拥有对应的序号。</li>
<li>Tensorflow 服务端(Tensorflow Server): 一个运行了<code>tf.train.Server</code>实例的进程，其为集群中的一员，并有主节点和工作节点之分。</li>
<li>工作节点的服务逻辑 (Worker service) : 其为一个可以使用本地设备对部分图进行计算的 RPC 逻辑，一个工作节点的服务逻辑实现了 worker_service.proto 接口， 所有的 TensorFlow 服务端均包含工作节点的服务逻辑。</li>
</ul>
<p>它们直接的关系图如下:</p>
<p><div align="left"><br>    <img src="http://p0.qhimg.com/t013054c6a49dce8f1b.png" width="800" height="450" alt="tensorflow"></div></p>
<p>Tensorflow使用<code>tf.train.ClusterSpec</code>来表示一个cluster。</p>
<p><div align="left"><br>    <img src="http://p6.qhimg.com/t01f70c812a6c8ae7b6.png" width="800" height="280" alt="tensorflow"></div></p>
<p>其中cluster接收一个map,并且map中包含了各个task所在host的主机地址，这个cluster共包含两类job: ps和worker。</p>
<p>创建好了cluster，需要创建各个task的server, 使用<code>tf.train.Server</code>函数，比如创建第一个worker的server:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server = tf.train.Server(cluster, <span class="attribute">job_name</span>=<span class="string">"worker"</span>, <span class="attribute">task_index</span>=0)</span><br></pre></td></tr></table></figure>
<p>上面各个task的server创建完成之后，就可以在构建Graph时使用<code>tf.device</code>来调用该cluster中的各个server并指定该<code>op</code>在CPU还是GPU上计算。具体如下面代码实例:</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">with <span class="keyword">tf</span>.device(<span class="string">"/job:ps/task:0"</span>):</span><br><span class="line">  weights_1 = <span class="keyword">tf</span>.Variable(...)</span><br><span class="line">  biases_1 = <span class="keyword">tf</span>.Variable(...)</span><br><span class="line"></span><br><span class="line">with <span class="keyword">tf</span>.device(<span class="string">"/job:ps/task:1"</span>):</span><br><span class="line">  weights_2 = <span class="keyword">tf</span>.Variable(...)</span><br><span class="line">  biases_2 = <span class="keyword">tf</span>.Variable(...)</span><br><span class="line"></span><br><span class="line">with <span class="keyword">tf</span>.device(<span class="string">"/job:worker/task:7"</span>):</span><br><span class="line">  <span class="built_in">input</span>, labels = ...</span><br><span class="line">  layer_1 = <span class="keyword">tf</span>.<span class="keyword">nn</span>.relu(<span class="keyword">tf</span>.matmul(<span class="built_in">input</span>, weights_1) + biases_1)</span><br><span class="line">  logits = <span class="keyword">tf</span>.<span class="keyword">nn</span>.relu(<span class="keyword">tf</span>.matmul(layer_1, weights_2) + biases_2)</span><br><span class="line">  # ...</span><br><span class="line">  train_op = ...</span><br><span class="line"></span><br><span class="line">with <span class="keyword">tf</span>.Session(<span class="string">"grpc://worker7.example.com:2222"</span>) <span class="keyword">as</span> ses<span class="variable">s:</span></span><br><span class="line">  <span class="keyword">for</span> _ in <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    sess.run(train_op)</span><br></pre></td></tr></table></figure>
<p>更多关于Tensorflow分布式训练的可以查看:<br><a href="https://www.tensorflow.org/deploy/distributed" target="_blank" rel="noopener">Distributed TensorFlow</a><br><a href="https://zhuanlan.zhihu.com/p/35083779" target="_blank" rel="noopener">分布式TensorFlow入门教程</a></p>
<p>下面是一个分布式训练的完成例子:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">FLAGS = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(_):</span><br><span class="line">  ps_hosts = FLAGS.ps_hosts.split(<span class="string">","</span>)</span><br><span class="line">  worker_hosts = FLAGS.worker_hosts.split(<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">  # Create a cluster <span class="keyword">from</span> the parameter<span class="built_in"> server </span><span class="keyword">and</span> worker hosts.</span><br><span class="line">  cluster = tf.train.ClusterSpec(&#123;<span class="string">"ps"</span>: ps_hosts, <span class="string">"worker"</span>: worker_hosts&#125;)</span><br><span class="line"></span><br><span class="line">  # Create <span class="keyword">and</span> start a<span class="built_in"> server </span><span class="keyword">for</span> the local task.</span><br><span class="line"> <span class="built_in"> server </span>= tf.train.Server(cluster,</span><br><span class="line">                           <span class="attribute">job_name</span>=FLAGS.job_name,</span><br><span class="line">                           <span class="attribute">task_index</span>=FLAGS.task_index)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> FLAGS.job_name == <span class="string">"ps"</span>:</span><br><span class="line">    server.join()</span><br><span class="line">  elif FLAGS.job_name == <span class="string">"worker"</span>:</span><br><span class="line"></span><br><span class="line">    # Assigns ops <span class="keyword">to</span> the local worker by default.</span><br><span class="line">    with tf.device(tf.train.replica_device_setter(</span><br><span class="line">        <span class="attribute">worker_device</span>=<span class="string">"/job:worker/task:%d"</span> % FLAGS.task_index,</span><br><span class="line">        <span class="attribute">cluster</span>=cluster)):</span><br><span class="line"></span><br><span class="line">      # Build model<span class="built_in">..</span>.</span><br><span class="line">      loss = <span class="built_in">..</span>.</span><br><span class="line">      global_step = tf.contrib.framework.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">      train_op = tf.train.AdagradOptimizer(0.01).minimize(</span><br><span class="line">          loss, <span class="attribute">global_step</span>=global_step)</span><br><span class="line"></span><br><span class="line">    # The StopAtStepHook handles stopping after running given steps.</span><br><span class="line">    hooks=[tf.train.StopAtStepHook(<span class="attribute">last_step</span>=1000000)]</span><br><span class="line"></span><br><span class="line">    # The MonitoredTrainingSession takes care of session initialization,</span><br><span class="line">    # restoring <span class="keyword">from</span> a checkpoint, saving <span class="keyword">to</span> a checkpoint, <span class="keyword">and</span> closing when done</span><br><span class="line">    # <span class="keyword">or</span> an <span class="builtin-name">error</span> occurs.</span><br><span class="line">    with tf.train.MonitoredTrainingSession(<span class="attribute">master</span>=server.target,</span><br><span class="line">                                           is_chief=(FLAGS.task_index == 0),</span><br><span class="line">                                           <span class="attribute">checkpoint_dir</span>=<span class="string">"/tmp/train_logs"</span>,</span><br><span class="line">                                           <span class="attribute">hooks</span>=hooks) as mon_sess:</span><br><span class="line">      <span class="keyword">while</span> <span class="keyword">not</span> mon_sess.should_stop():</span><br><span class="line">        # <span class="builtin-name">Run</span> a training <span class="keyword">step</span> asynchronously.</span><br><span class="line">        # See &lt;a <span class="attribute">href</span>=<span class="string">"./../api_docs/python/tf/train/SyncReplicasOptimizer"</span>&gt;&lt;code&gt;tf.train.SyncReplicasOptimizer&lt;/code&gt;&lt;/a&gt; <span class="keyword">for</span> additional details on how <span class="keyword">to</span></span><br><span class="line">        # perform *synchronous* training.</span><br><span class="line">        # mon_sess.<span class="builtin-name">run</span> handles AbortedError <span class="keyword">in</span> case of preempted PS.</span><br><span class="line">        mon_sess.<span class="builtin-name">run</span>(train_op)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  parser = argparse.ArgumentParser()</span><br><span class="line">  parser.register(<span class="string">"type"</span>, <span class="string">"bool"</span>, lambda v: v.lower() == <span class="string">"true"</span>)</span><br><span class="line">  # Flags <span class="keyword">for</span> defining the tf.train.ClusterSpec</span><br><span class="line">  parser.add_argument(</span><br><span class="line">      <span class="string">"--ps_hosts"</span>,</span><br><span class="line">      <span class="attribute">type</span>=str,</span><br><span class="line">      <span class="attribute">default</span>=<span class="string">""</span>,</span><br><span class="line">      <span class="attribute">help</span>=<span class="string">"Comma-separated list of hostname:port pairs"</span></span><br><span class="line">  )</span><br><span class="line">  parser.add_argument(</span><br><span class="line">      <span class="string">"--worker_hosts"</span>,</span><br><span class="line">      <span class="attribute">type</span>=str,</span><br><span class="line">      <span class="attribute">default</span>=<span class="string">""</span>,</span><br><span class="line">      <span class="attribute">help</span>=<span class="string">"Comma-separated list of hostname:port pairs"</span></span><br><span class="line">  )</span><br><span class="line">  parser.add_argument(</span><br><span class="line">      <span class="string">"--job_name"</span>,</span><br><span class="line">      <span class="attribute">type</span>=str,</span><br><span class="line">      <span class="attribute">default</span>=<span class="string">""</span>,</span><br><span class="line">      <span class="attribute">help</span>=<span class="string">"One of 'ps', 'worker'"</span></span><br><span class="line">  )</span><br><span class="line">  # Flags <span class="keyword">for</span> defining the tf.train.Server</span><br><span class="line">  parser.add_argument(</span><br><span class="line">      <span class="string">"--task_index"</span>,</span><br><span class="line">      <span class="attribute">type</span>=int,</span><br><span class="line">      <span class="attribute">default</span>=0,</span><br><span class="line">      <span class="attribute">help</span>=<span class="string">"Index of task within the job"</span></span><br><span class="line">  )</span><br><span class="line">  FLAGS, unparsed = parser.parse_known_args()</span><br><span class="line">  tf.app.<span class="builtin-name">run</span>(<span class="attribute">main</span>=main, argv=[sys.argv[0]] + unparsed)</span><br></pre></td></tr></table></figure>
<p>使用<code>trainer.py</code>脚本开启两个ps和两个worker，具体操作如下:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># On ps0.example.com:</span></span><br><span class="line">$ python trainer.py \</span><br><span class="line">     <span class="attribute">--ps_hosts</span>=ps0.example.com:2222,ps1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--worker_hosts</span>=worker0.example.com:2222,worker1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--job_name</span>=ps <span class="attribute">--task_index</span>=0</span><br><span class="line"><span class="comment"># On ps1.example.com:</span></span><br><span class="line">$ python trainer.py \</span><br><span class="line">     <span class="attribute">--ps_hosts</span>=ps0.example.com:2222,ps1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--worker_hosts</span>=worker0.example.com:2222,worker1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--job_name</span>=ps <span class="attribute">--task_index</span>=1</span><br><span class="line"><span class="comment"># On worker0.example.com:</span></span><br><span class="line">$ python trainer.py \</span><br><span class="line">     <span class="attribute">--ps_hosts</span>=ps0.example.com:2222,ps1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--worker_hosts</span>=worker0.example.com:2222,worker1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--job_name</span>=worker <span class="attribute">--task_index</span>=0</span><br><span class="line"><span class="comment"># On worker1.example.com:</span></span><br><span class="line">$ python trainer.py \</span><br><span class="line">     <span class="attribute">--ps_hosts</span>=ps0.example.com:2222,ps1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--worker_hosts</span>=worker0.example.com:2222,worker1.example.com:2222 \</span><br><span class="line">     <span class="attribute">--job_name</span>=worker <span class="attribute">--task_index</span>=1</span><br></pre></td></tr></table></figure>
<h2 id="Tensorflow结合kubernetes进行分布式训练"><a href="#Tensorflow结合kubernetes进行分布式训练" class="headerlink" title="Tensorflow结合kubernetes进行分布式训练"></a>Tensorflow结合kubernetes进行分布式训练</h2><p>上面简单的介绍了下Tensorflow的基础知识以及Tensorflow是如何进行分布式训练的。但是对于资源的隔离和管理以及调度等场景还是不能很好的解决，接下来介绍下基于kubeflow对Tensorflow的模型进行大规模的分布式训练。</p>
<h4 id="kubeflow安装"><a href="#kubeflow安装" class="headerlink" title="kubeflow安装"></a>kubeflow安装</h4><p>之前的文章已经介绍了，这里就不详细介绍，具体的安装请看之前的文章:<a href="https://xigang.github.io/2018/12/08/kubeflow-intro/" target="_blank" rel="noopener">部署kubeflow环境</a></p>
<h4 id="分布式Tensorflow与Kubenetes结合"><a href="#分布式Tensorflow与Kubenetes结合" class="headerlink" title="分布式Tensorflow与Kubenetes结合"></a>分布式Tensorflow与Kubenetes结合</h4><p>使用kubernetes对机器学习的训练任务进行管理的好处:</p>
<p>集群管理:</p>
<ul>
<li>计算资源调度支持- CPU和GPU</li>
<li>训练任务生命周期管理- CRD&amp;Controller</li>
<li>资源隔离 - Linux Namespace</li>
<li>监控，日志，告警 - 容器云平台提供的能力</li>
</ul>
<p>网络:</p>
<ul>
<li>服务发现 - Service(headless)</li>
</ul>
<p>存储:</p>
<ul>
<li><p>存储持久化 - PV&amp;PVC&amp;CSI</p>
<p><div align="left"><br>  <img src="http://p4.qhimg.com/t01c0f1789cdc07228c.png" width="800" height="420" alt="tensorflow"></div></p>
</li>
</ul>
<h4 id="TFJob"><a href="#TFJob" class="headerlink" title="TFJob"></a>TFJob</h4><p><code>TFJob</code>是kubernetes的一个<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener">自定义资源</a>使你能够运行Tensorflow的训练任务在kubernetes上。kubeflow实现的<code>TfJob</code>是<a href="https://github.com/kubeflow/tf-operator">tf-operator</a>项目。</p>
<p>TFJob是kubernetes一个对象资源，定义的格式和其它的kubernetes资源一样。具体的实例如下:</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">apiVersion</span>: <span class="string">"kubeflow.org/v1alpha2"</span></span><br><span class="line"><span class="attribute">kind</span>: <span class="string">"TFJob"</span></span><br><span class="line"><span class="attribute">metadata</span>:</span><br><span class="line">  <span class="attribute">name</span>: <span class="string">"dist-mnist-for-e2e-test"</span></span><br><span class="line"><span class="attribute">spec</span>:</span><br><span class="line">  <span class="attribute">tfReplicaSpecs</span>:</span><br><span class="line">    <span class="attribute">PS</span>:</span><br><span class="line">      <span class="attribute">replicas</span>: <span class="number">1</span>  # 定义一个ps</span><br><span class="line">      <span class="attribute">restartPolicy</span>: Never</span><br><span class="line">      <span class="attribute">template</span>:</span><br><span class="line">        <span class="attribute">spec</span>:</span><br><span class="line">          <span class="attribute">containers</span>:</span><br><span class="line">            - <span class="attribute">name</span>: tensorflow</span><br><span class="line">              <span class="attribute">image</span>: xigang/<span class="attribute">tf-mnist-distributed</span>:gpu</span><br><span class="line">              <span class="attribute">command</span>: [<span class="string">"python"</span>, <span class="string">"/app/main.py"</span>]</span><br><span class="line">              <span class="attribute">ports</span>:</span><br><span class="line">              - <span class="attribute">containerPort</span>: <span class="number">2222</span></span><br><span class="line">                <span class="attribute">name</span>: tfjob-port</span><br><span class="line">          <span class="attribute">restartPolicy</span>: OnFailure</span><br><span class="line">    <span class="attribute">Worker</span>:</span><br><span class="line">      <span class="attribute">replicas</span>: <span class="number">2</span> # 定义两个worker</span><br><span class="line">      <span class="attribute">restartPolicy</span>: Never</span><br><span class="line">      <span class="attribute">template</span>:</span><br><span class="line">        <span class="attribute">spec</span>:</span><br><span class="line">          <span class="attribute">containers</span>:</span><br><span class="line">            - <span class="attribute">name</span>: tensorflow</span><br><span class="line">              <span class="attribute">image</span>: xigang/<span class="attribute">tf-mnist-distributed</span>:gpu</span><br><span class="line">              <span class="attribute">command</span>: [<span class="string">"python"</span>, <span class="string">"/app/main.py"</span>]</span><br><span class="line">              <span class="attribute">ports</span>:</span><br><span class="line">              - <span class="attribute">containerPort</span>: <span class="number">2222</span></span><br><span class="line">                <span class="attribute">name</span>: tfjob-port</span><br><span class="line">              <span class="attribute">imagePullPolicy</span>: Always</span><br><span class="line">              <span class="attribute">resources</span>:</span><br><span class="line">                <span class="attribute">limits</span>:</span><br><span class="line">                  nvidia.com/<span class="attribute">gpu</span>: <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>通过上面的yaml文件，我们就能在kubernetes上创建我们的训练任务了。执行下面的命令:</p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># kubectl  create -f mnist_multi_gpu.yaml</span></span><br><span class="line">tfjob.kubeflow.org/dist-mnist-<span class="keyword">for</span>-e2e-test created</span><br></pre></td></tr></table></figure>
<p>查看tfjobs任务是否创建成功，并且状态是否处于ready状态:</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl <span class="built_in">get</span> tfjobs</span><br><span class="line">NAME                      AGE</span><br><span class="line"><span class="built_in">dist</span>-mnist-<span class="keyword">for</span>-e2e-test   <span class="number">47</span>s</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get pods</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">dist-mnist-for-e2e-test-ps<span class="number">-0</span>       <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">53</span>s</span><br><span class="line">dist-mnist-for-e2e-test-worker<span class="number">-0</span>   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">53</span>s</span><br><span class="line">dist-mnist-for-e2e-test-worker<span class="number">-1</span>   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">53</span>s</span><br></pre></td></tr></table></figure>
<p>一切正常的情况下，查看worker的训练日志:</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># docker logs -f 265d4ae426d6</span><br><span class="line">/usr/<span class="built_in">local</span>/lib/python2.7/dist-packages/h5py/__init__.py:<span class="number">36</span>: FutureWarning: Conversion of the <span class="built_in">second</span> argument of issubdtype from `<span class="built_in">float</span>` to `<span class="built_in">np</span>.floating` <span class="built_in">is</span> deprecated. In future, it will be treated as `<span class="built_in">np</span>.float64 == <span class="built_in">np</span>.dtype(<span class="built_in">float</span>).type`.</span><br><span class="line">  from ._conv import register_converters as _register_converters</span><br><span class="line"><span class="number">2019</span>-<span class="number">01</span>-<span class="number">30</span> <span class="number">07</span>:<span class="number">37</span>:<span class="number">21.960547</span>: I tensorflow/core/platform/cpu_feature_guard.cc:<span class="number">137</span>] Your CPU supports instructions that this TensorFlow binary was <span class="keyword">not</span> compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class="line"><span class="number">2019</span>-<span class="number">01</span>-<span class="number">30</span> <span class="number">07</span>:<span class="number">37</span>:<span class="number">22.285116</span>: I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">1105</span>] Found device <span class="number">0</span> with <span class="built_in">properties</span>:</span><br><span class="line">name: Tesla K80 major: <span class="number">3</span> <span class="built_in">minor</span>: <span class="number">7</span> memoryClockRate(GHz): <span class="number">0.8235</span></span><br><span class="line">pciBusID: <span class="number">0000</span>:<span class="number">05</span>:<span class="number">00.0</span></span><br><span class="line">totalMemory: <span class="number">11.</span>92GiB freeMemory: <span class="number">11.</span>68GiB</span><br><span class="line"><span class="number">2019</span>-<span class="number">01</span>-<span class="number">30</span> <span class="number">07</span>:<span class="number">37</span>:<span class="number">22.285154</span>: I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">1195</span>] Creating TensorFlow device (/device:GPU:<span class="number">0</span>) -&gt; (device: <span class="number">0</span>, name: Tesla K80, pci bus id: <span class="number">0000</span>:<span class="number">05</span>:<span class="number">00.0</span>, compute capability: <span class="number">3.7</span>)</span><br><span class="line"><span class="number">2019</span>-<span class="number">01</span>-<span class="number">30</span> <span class="number">07</span>:<span class="number">37</span>:<span class="number">27.471978</span>: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:<span class="number">215</span>] Initialize GrpcChannelCache <span class="keyword">for</span> job ps -&gt; &#123;<span class="number">0</span> -&gt; dist-mnist-<span class="keyword">for</span>-e2e-test-ps-<span class="number">0</span>:<span class="number">2222</span>&#125;</span><br><span class="line"><span class="number">2019</span>-<span class="number">01</span>-<span class="number">30</span> <span class="number">07</span>:<span class="number">37</span>:<span class="number">27.472009</span>: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:<span class="number">215</span>] Initialize GrpcChannelCache <span class="keyword">for</span> job worker -&gt; &#123;<span class="number">0</span> -&gt; localhost:<span class="number">2222</span>, <span class="number">1</span> -&gt; dist-mnist-<span class="keyword">for</span>-e2e-test-worker-<span class="number">1</span>:<span class="number">2222</span>&#125;</span><br><span class="line"><span class="number">2019</span>-<span class="number">01</span>-<span class="number">30</span> <span class="number">07</span>:<span class="number">37</span>:<span class="number">27.474101</span>: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:<span class="number">324</span>] Started server with target: grpc://localhost:<span class="number">2222</span></span><br><span class="line">WARNING:tensorflow:From /app/main.py:<span class="number">202</span>: __init__ (from tensorflow.python.training.supervisor) <span class="built_in">is</span> deprecated <span class="keyword">and</span> will be removed <span class="keyword">in</span> a future version.</span><br><span class="line">Instructions <span class="keyword">for</span> updating:</span><br><span class="line">Please switch to tf.train.MonitoredTrainingSession</span><br><span class="line"><span class="number">2019</span>-<span class="number">01</span>-<span class="number">30</span> <span class="number">07</span>:<span class="number">37</span>:<span class="number">28.924878</span>: I tensorflow/core/distributed_runtime/master_session.cc:<span class="number">1017</span>] Start master session 23fe02df8676c506 with config:</span><br><span class="line">Worker <span class="number">0</span>: Initializing session...</span><br><span class="line">Extracting /train/tensorflow/input_data/train-images-idx3-ubyte.gz</span><br><span class="line">Extracting /train/tensorflow/input_data/train-<span class="built_in">labels</span>-idx1-ubyte.gz</span><br><span class="line">Extracting /train/tensorflow/input_data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting /train/tensorflow/input_data/t10k-<span class="built_in">labels</span>-idx1-ubyte.gz</span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">0</span>: <span class="number">0.1445</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">10</span>: <span class="number">0.7489</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">20</span>: <span class="number">0.8691</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">30</span>: <span class="number">0.8885</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">40</span>: <span class="number">0.895</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">50</span>: <span class="number">0.8975</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">60</span>: <span class="number">0.9098</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">70</span>: <span class="number">0.9185</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">80</span>: <span class="number">0.9229</span></span><br><span class="line">Accuracy <span class="built_in">at</span> <span class="keyword">step</span> <span class="number">90</span>: <span class="number">0.9283</span></span><br><span class="line">Adding run metadata <span class="keyword">for</span> <span class="number">99</span></span><br></pre></td></tr></table></figure>
<h4 id="Tensorflow是如何与kubernetes结合"><a href="#Tensorflow是如何与kubernetes结合" class="headerlink" title="Tensorflow是如何与kubernetes结合?"></a>Tensorflow是如何与kubernetes结合?</h4><p>上面已经描述关于Tensorflow的分布式训练的基本原理，需要指定<code>tf.train.ClusterSpec</code>并基于各个task创建<code>tf.train.Server</code>。但是需要指定相关的host和index信息，但是使用kubernetes进行调度时，由于Pod的IP会从网络池中获取，事先是不知道的。而kuberentes的做法是在容器创建成功之后将Tensorflow分布式训练需要的相关信息已环境变量的方式注入到容器中，这时当容器内部的task执行时，就会通过<code>TF_CONFIG</code>获取相关的环境变量并解析该<code>TF_CONFIG</code>环境变量进行后续的处理。</p>
<p>我们exec进入到worker容器中会发现一个<code>TF_CONFIG</code>的环境变量:</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">TF_CONFIG=&#123;</span><br><span class="line">	<span class="string">"cluster"</span>:&#123;</span><br><span class="line">		<span class="string">"ps"</span>:[<span class="string">"dist-mnist-for-e2e-test-ps-0:2222"</span>],</span><br><span class="line">		<span class="string">"worker"</span>:[<span class="string">"dist-mnist-for-e2e-test-worker-0:2222"</span>,<span class="string">"dist-mnist-for-e2e-test-worker-1:2222"</span>]</span><br><span class="line">		 &#125;,</span><br><span class="line">    <span class="string">"task"</span>:&#123;</span><br><span class="line">        <span class="string">"type"</span>:<span class="string">"worker"</span>,</span><br><span class="line">        <span class="string">"index"</span>:<span class="number">0</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"environment"</span>:<span class="string">"cloud"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而需要训练的模型通过该<code>TF_CONFIG</code>环境变量获取相关的信息,下面是mnist模型获取<code>TF_CONFIG</code>信息的部分代码片段:</p>
<p> <div align="left"><br>        <img src="http://p1.qhimg.com/t01801baaabff3603cf.png" width="700" height="500" alt="tensorflow"></div></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.tensorflow.org/guide/graphs" target="_blank" rel="noopener">https://www.tensorflow.org/guide/graphs</a><br><a href="https://www.tensorflow.org/deploy/distributed" target="_blank" rel="noopener">https://www.tensorflow.org/deploy/distributed</a><br><a href="https://www.kubeflow.org/docs/components/tftraining/" target="_blank" rel="noopener">https://www.kubeflow.org/docs/components/tftraining/</a><br><a href="https://zhuanlan.zhihu.com/p/35083779" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35083779</a><br><a href="https://www.jiqizhixin.com/articles/2017-12-10-5" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-12-10-5</a><br><a href="https://blog.csdn.net/geyunfei_/article/details/78782804" target="_blank" rel="noopener">https://blog.csdn.net/geyunfei_/article/details/78782804</a></p>
</div><div class="tags"><a href="/tags/kubernetes/">kubernetes</a><a href="/tags/kubeflow/">kubeflow</a><a href="/tags/tensorflow/">tensorflow</a></div><div class="post-nav"><a class="pre" href="/2019/02/17/gang-scheduler/">适合AI场景的调度器 - Gang-Schedule</a><a class="next" href="/2019/01/17/mxnet/">MXNet结合kubeflow进行分布式训练</a></div><div id="container"></div><link rel="stylesheet" type="text/css" href="//unpkg.com/gitalk/dist/gitalk.css?v=0.0.0"><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js?v=0.0.0"></script><script type="text/javascript" src="//unpkg.com/gitalk/dist/gitalk.min.js?v=0.0.0"></script><script>var gitalk = new Gitalk({
  clientID: '86d1945e3a9358946043',
  clientSecret: '304f48ee3394ae5dab75d19a966506e170d850f6',
  repo: 'xigang.github.io',
  owner: 'xigang',
  admin: ['xigang'],
  id: md5(location.pathname),
  distractionFreeMode: false
})
gitalk.render('container')
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="https://github.com/xigang"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/docker/">docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/etcd/">etcd</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kubernetes/">kubernetes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/microservices/">microservices</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/日志监控/">日志监控</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/etcd/" style="font-size: 15px;">etcd</a> <a href="/tags/kuernetes/" style="font-size: 15px;">kuernetes</a> <a href="/tags/ceph/" style="font-size: 15px;">ceph</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/kubeflow/" style="font-size: 15px;">kubeflow</a> <a href="/tags/scheduler/" style="font-size: 15px;">scheduler</a> <a href="/tags/kubernetes/" style="font-size: 15px;">kubernetes</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/microservices/" style="font-size: 15px;">microservices</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/prometheus/" style="font-size: 15px;">prometheus</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/02/17/gang-scheduler/">适合AI场景的调度器 - Gang-Schedule</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/30/tensorflow/">Tensorflow结合kubeflow进行分布式训练</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/17/mxnet/">MXNet结合kubeflow进行分布式训练</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/31/Orphaned-pod/">定位 Orphaned Pod Found - but Volume Paths Are Still Present on Disk 问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/14/prometheus/">记一次InfoQ采访 <<360容器平台监控实践>></a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/08/kubeflow-intro/">Kubeflow使用Kubernetes进行机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/24/kube-dns/">Kubernetes DNS 介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/08/nvidia-container-runtime/">深入理解 Nvidia-docker2.0</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/05/nvidia-docker2/">Nvidia-Docker2在kubernetes上实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/14/namespace-md/">浅谈 Linux Namespace</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://gogap.cn/" title="gogap" target="_blank">gogap</a><ul></ul><a href="http://www.0x7c00.net/" title="31744" target="_blank">31744</a><ul></ul><a href="https://www.opsdev.cn/" title="360opsdev" target="_blank">360opsdev</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">xigang's home.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.3.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>